% A framework for dynamic graph representation learning
% This is a tensorflow implementation of our paper. If you use our code in your research, please cite our paper accordingly [1]
% Developer: Aynaz Taheri
% Date: Dec 2019
% Copyright Â© 2019 University of Illinois at Chicago.
% This framework is an extension of the Gated Graph Neural Network (GGNN) [2], see https://github.com/microsoft/gated-graph-neural-network-samples

The code is the implementation of a dynamic graph autoencoder, which includes an LSTM encode-decoder model and a gated graph neural network.
We leverage the GGNNs potential for learning the topology of the graph. GGNNs are embedded in a recurrent encoder to preserve the topology of a dynamic graph at each time step.
The decoder is supposed to reconstruct the dynamics of the graph observed by the encoder.

main.py is the main file that should be run. The input is a sequence of graph snapshots. Each graph file is represented by an adjacency list.
This code was tested with Python 3.5 and Tensorflow 1.11.0

To run the main.py, initialize the below arguments:
"-w",  "Window size"
"-b", "Batch size"
"-s", "lstm hidden size"
"-n", "Number of encoder-decoder layers"
"-l", "Learning rate"
"-e", "Number of epochs"
"-t", "Training data directory"
"-v", "Test data directory"
"-p", "Training helper"
"-d", "Keep prob dropout, if 1 no dropout"
"-o", "the overlap that we need to slid the window; it should be less than window size"


The code for the paper is at [1]
[1] Taheri, Aynaz and Berger-Wolf, Tanya, Proceedings of the International Conference on Advances in Social Networks Analysis and Mining, 2019
[2] Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel, Gated Graph Sequence Neural Networks. ICLR 2016

